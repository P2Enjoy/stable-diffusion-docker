version: '3.9'

x-gpu-base-service: &gpu_service
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            device_ids: ['0']
            capabilities: [gpu]

x-base_service: &base_service
  ports:
    - "7860:7860"
  build:
      context: ./services/AUTOMATIC1111
  volumes:
    - &v1 ./data:/data
    - &v2 ./output:/output
  deploy:
    restart_policy:
      delay: 5s
      max_attempts: 10
      window: 120s

name: webui-docker

services:
  download:
    build: ./services/download/
    profiles: ["download"]
    volumes:
      - *v1

  xformers:
    <<: *gpu_service
    build: ./services/xformers/
    profiles: ["xformers"]
    volumes:
      - ./services/xformers/data:/deploy

  tensorflow:
    <<: *gpu_service
    build: ./services/cuDNN/
    profiles: ["tensorflow"]
    volumes:
      - ./services/cuDNN/data:/deploy
    stdin_open: true
    tty: true

  auto: &automatic
    <<: *base_service
    <<: *gpu_service
    profiles: ["auto"]
    environment:
      - CLI_ARGS=--allow-code --medvram --xformers --gradio-img2img-tool color-sketch --deepdanbooru --opt-split-attention --opt-split-attention-invokeai --opt-channelslast #--ngrok pyngrok
      - RUN_ARGS='/docker/run.sh -u ${ROOT}/webui.py --listen --port 7860 --ckpt-dir ${ROOT}/models/Stable-diffusion --disable-safe-unpickle ${CLI_ARGS}'
      
  auto_debug:
    <<: *automatic
    profiles: ["auto_debug"]
    stdin_open: true
    tty: true
    environment:
      - RUN-ARGS=/bin/bash

  auto-cpu:
    <<: *base_service
    profiles: ["auto-cpu"]
    environment:
      - CLI_ARGS=--no-half --precision full --gradio-img2img-tool color-sketch
