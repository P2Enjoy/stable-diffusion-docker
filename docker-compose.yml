version: '3.9'

x-gpu-base-service: &gpu_service
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            device_ids: ['0']
            capabilities: [gpu]

x-base_service: &base_service
  ports:
    - "7860:7860"
  build:
      context: ./services/AUTOMATIC1111
  volumes:
    - &v1 ./data:/data
    - &v2 ./output:/output
  deploy:
    restart_policy:
      delay: 5s
      max_attempts: 10
      window: 120s

name: webui-docker

services:
  download:
    build: ./services/download/
    profiles: ["download"]
    volumes:
      - *v1

  xformers:
    <<: *gpu_service
    build: ./services/xformers/
    profiles: ["xformers"]
    volumes:
      - ./services/xformers/data:/deploy

  tensorflow:
    <<: *gpu_service
    build: ./services/tensorflow/
    profiles: ["tensorflow"]
    environment:
      - NVCC_FLAGS="--use_fast_math -allow-unsupported-compiler" 
    volumes:
      - ./services/tensorflow/data:/deploy

  auto: &automatic
    <<: *base_service
    <<: *gpu_service
    profiles: ["auto"]
    environment:
      - TORCH_COMMAND=pip install /docker/tensorflow-*.whl /docker/torch-*.whl /docker/torchvision-*.whl --extra-index-url https://download.pytorch.org/whl/cu116
      - CLI_ARGS=--medvram --xformers --deepdanbooru --gradio-img2img-tool color-sketch --opt-split-attention --opt-split-attention-invokeai --opt-channelslast
      - RUN_ARGS=/docker/run.sh -u /stable-diffusion-webui/webui.py --listen --enable-insecure-extension-access --port 7860 --allow-code
      
  auto_debug:
    <<: *automatic
    profiles: ["auto_debug"]
    stdin_open: true
    tty: true
    environment:
      - RUN_ARGS=/docker/debug.sh

  auto-cpu:
    <<: *base_service
    profiles: ["auto-cpu"]
    environment:
      - CLI_ARGS=--no-half --precision full --gradio-img2img-tool color-sketch
